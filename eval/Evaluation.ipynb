{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Frames in Semantic Space\n",
    "This notebook is used to analyze whether the semantic vector spaces generated through sentence embeddings can capture semantic frames well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import operator\n",
    "import multiprocessing as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMENET_DATA_DIR = \"../data/parsed_data/parsed_framenet.p\"\n",
    "EMBEDDINGS_DATA_DIR = \"./embeddings\"\n",
    "ANALYSIS_DIR = \"./analysis\"\n",
    "\n",
    "N_CORES = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import FrameNet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>phrase_type</th>\n",
       "      <th>semantic_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>enron-thread-159550</td>\n",
       "      <td>I have completed the invoices for April, May a...</td>\n",
       "      <td>complete</td>\n",
       "      <td>verb</td>\n",
       "      <td>Activity_finish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>enron-thread-159550</td>\n",
       "      <td>I have completed the invoices for April, May a...</td>\n",
       "      <td>May</td>\n",
       "      <td>noun</td>\n",
       "      <td>Calendric_unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>enron-thread-159550</td>\n",
       "      <td>I have completed the invoices for April, May a...</td>\n",
       "      <td>June</td>\n",
       "      <td>noun</td>\n",
       "      <td>Calendric_unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>enron-thread-159550</td>\n",
       "      <td>I have completed the invoices for April, May a...</td>\n",
       "      <td>month</td>\n",
       "      <td>noun</td>\n",
       "      <td>Calendric_unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>enron-thread-159550</td>\n",
       "      <td>I have completed the invoices for April, May a...</td>\n",
       "      <td>total</td>\n",
       "      <td>noun</td>\n",
       "      <td>Amounting_to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          corpus             document  \\\n",
       "0  LUCorpus-v0.3  enron-thread-159550   \n",
       "1  LUCorpus-v0.3  enron-thread-159550   \n",
       "2  LUCorpus-v0.3  enron-thread-159550   \n",
       "3  LUCorpus-v0.3  enron-thread-159550   \n",
       "4  LUCorpus-v0.3  enron-thread-159550   \n",
       "\n",
       "                                            sentence      word phrase_type  \\\n",
       "0  I have completed the invoices for April, May a...  complete        verb   \n",
       "1  I have completed the invoices for April, May a...       May        noun   \n",
       "2  I have completed the invoices for April, May a...      June        noun   \n",
       "3  I have completed the invoices for April, May a...     month        noun   \n",
       "4  I have completed the invoices for April, May a...     total        noun   \n",
       "\n",
       "    semantic_frame  \n",
       "0  Activity_finish  \n",
       "1   Calendric_unit  \n",
       "2   Calendric_unit  \n",
       "3   Calendric_unit  \n",
       "4     Amounting_to  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle(FRAMENET_DATA_DIR)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings for different models\n",
    "Pre-trained models can be found on [Github](https://github.com/UKPLab/sentence-transformers#pretrained-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(embedding_dir, model_name, model, sentences):\n",
    "    \"\"\"\n",
    "    Creates and saves out embeddings for a given model, if embeddings are not found.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dir (string): directory where embedding files should be found.\n",
    "        model_name (string): name of model to use to generate embeddings.\n",
    "        model (SentneceTransformer): model to generate embeddings with.\n",
    "        sentences (list of string): list of sentences to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        (Pandas DataFrame): Pandas DataFrame containing two columns: sentences, and embeddings.\n",
    "    \"\"\"\n",
    "    # check if embedding file for model already exists\n",
    "    embedding_filename = \"{model}_embeddings.p\".format(model=model_name)\n",
    "    embedding_filepath = os.path.join(embedding_dir, embedding_filename)\n",
    "    \n",
    "    if os.path.isfile(embedding_filepath) and os.access(embedding_filepath, os.R_OK):\n",
    "        print(\"Embeddings for {model_name} already exist at {file_path}...loading data.\".format(model_name=model_name,\n",
    "                                                                                                file_path=embedding_filepath))\n",
    "        return pd.read_pickle(embedding_filepath)\n",
    "    \n",
    "    # generate embeddings for sentences\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # create dataframe from sentences and embeddings\n",
    "    embeddings_df = pd.DataFrame({\n",
    "        \"sentence\": sentences,\n",
    "        \"embedding\": embeddings\n",
    "    })\n",
    "    \n",
    "    # save to file\n",
    "    embeddings_df.to_pickle(embedding_filepath)\n",
    "    \n",
    "    # return dataframe\n",
    "    return embeddings_df\n",
    "\n",
    "def get_embeddings(embedding_dir, model_name):\n",
    "    \"\"\"\n",
    "    Returns embeddings for specified model name, or None if not there.\n",
    "    \n",
    "    Inputs:\n",
    "        embedding_dir (string): directory where embedding files should be found.\n",
    "        model_name (string): name of model to fetch embeddings for.\n",
    "        \n",
    "    Output:\n",
    "        (Pandas DataFrame): Pandas DataFrame containing two columns: sentences, and embeddings.\n",
    "    \"\"\"\n",
    "    # check if embedding file for model already exists\n",
    "    embedding_filename = \"{model}_embeddings.p\".format(model=model_name)\n",
    "    embedding_filepath = os.path.join(embedding_dir, embedding_filename)\n",
    "    \n",
    "    if os.path.isfile(embedding_filepath) and os.access(embedding_filepath, os.R_OK):\n",
    "        return pd.read_pickle(embedding_filepath)\n",
    "    \n",
    "    print(\"Embeddinds for {model_name} not found at {file_path}.\".format(model_name=model_name, file_path=embedding_filepath))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for bert-base-nli-mean-tokens already exist at ./embeddings/bert-base-nli-mean-tokens_embeddings.p...loading data.\n",
      "Embeddings for bert-large-nli-mean-tokens already exist at ./embeddings/bert-large-nli-mean-tokens_embeddings.p...loading data.\n",
      "Embeddings for bert-base-nli-stsb-mean-tokens already exist at ./embeddings/bert-base-nli-stsb-mean-tokens_embeddings.p...loading data.\n",
      "Embeddings for bert-large-nli-stsb-mean-tokens already exist at ./embeddings/bert-large-nli-stsb-mean-tokens_embeddings.p...loading data.\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings using all models, if they don't already exist\n",
    "bert_models = [\"bert-base-nli-mean-tokens\", \"bert-large-nli-mean-tokens\", \"bert-base-nli-stsb-mean-tokens\", \"bert-large-nli-stsb-mean-tokens\"]\n",
    "sentences = data_df[\"sentence\"].unique()\n",
    "\n",
    "for model_str in bert_models:\n",
    "    generate_embeddings(EMBEDDINGS_DATA_DIR, model_str, SentenceTransformer(model_str), sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_nli_mean_tokens_embedding_df = get_embeddings(EMBEDDINGS_DATA_DIR, \"bert-base-nli-mean-tokens\")\n",
    "bert_large_nli_mean_tokens_embedding_df = get_embeddings(EMBEDDINGS_DATA_DIR, \"bert-large-nli-mean-tokens\")\n",
    "bert_base_nli_stsb_mean_tokens_embedding_df = get_embeddings(EMBEDDINGS_DATA_DIR, \"bert-base-nli-stsb-mean-tokens\")\n",
    "bert_large_nli_stsb_mean_tokens_embedding_df = get_embeddings(EMBEDDINGS_DATA_DIR, \"bert-large-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = [\"bert-base-nli-mean-tokens\", \"bert-large-nli-mean-tokens\", \"bert-base-nli-stsb-mean-tokens\", \"bert-large-nli-stsb-mean-tokens\"]\n",
    "model_df_dict = { name: get_embeddings(EMBEDDINGS_DATA_DIR, name) for name in bert_models }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_cluster_analysis(X, labels, metric):\n",
    "    \"\"\"\n",
    "    Performs intra cluster analysis, returning avg and max of distances of points within a cluster.\n",
    "    \n",
    "    Args:\n",
    "        X: list of all sentences\n",
    "        labels: list of all cluster labels corresponding to the sentence in X \n",
    "            (examples: [\"ANC\", \"Miscellaneous\", ...] OR [<list of semantic frames>]\n",
    "        metric: metric string as understood by sklearn (manhattan / cosine / euclidean)\n",
    "\n",
    "    Returns:\n",
    "        dict with {cluster_label : {avg, max}}\n",
    "    \"\"\"\n",
    "    # make sure labels are unique so we don't duplicate computation\n",
    "    unique_labels = np.unique(labels)\n",
    "    result = {label : {} for label in unique_labels}\n",
    "    \n",
    "    # add all sentences that fit a label\n",
    "    classified_sentences = {label : [] for label in unique_labels}\n",
    "    for x, label in zip(X, labels):\n",
    "        classified_sentences[label].append(x)\n",
    "        \n",
    "    # compute intra-cluster distances\n",
    "    for label, sentences in classified_sentences.items():\n",
    "        # pairwise distance calculation\n",
    "        distances = pairwise_distances(sentences, sentences, metric=metric, n_jobs=-1)\n",
    "        \n",
    "        # compute average diameter distance\n",
    "        n_sentences = len(sentences)\n",
    "        num_combinations = n_sentences * (n_sentences - 1)\n",
    "        result[label][\"avg\"] = np.sum(distances) / num_combinations\n",
    "        \n",
    "        # compute complute diameter distance\n",
    "        result[label][\"max\"] = np.max(distances)\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_cluster_similarity(framenet_df, embeddings_df, filter_col, metric, output_dir, output_filename):\n",
    "    \"\"\"\n",
    "    Computes measures of cluster similarity within and between corpuses.\n",
    "    \n",
    "    Args: \n",
    "        framenet_df (Pandas DataFrame): data from FrameNet that contains (1) corpus; (2) documents; (3) semantic frames; and (4) sentences.\n",
    "        embeddings_df (Pandas DataFrame): sentences and their embeddings.\n",
    "        filter_col (string): column to do clustering on.\n",
    "        metric (string): distane meteric to use when computing similarity as understood by sklearn (manhattan / cosine / euclidean).\n",
    "        output_dir (string): directory to save analysis file to.\n",
    "        output_filepath (string): filename to save csv to.\n",
    "    \n",
    "    Returns:\n",
    "        (Pandas DataFrame): DataFrame of evaluation metrics for each cluster.\n",
    "    \"\"\"\n",
    "    # create a DataFrame with corpus, sentences, and embeddings\n",
    "    data_df = pd.merge(framenet_df[[filter_col, \"sentence\"]].drop_duplicates(),\n",
    "                       embeddings_df, on=\"sentence\", how=\"inner\")\n",
    "    X = np.vstack(data_df[\"embedding\"].values)\n",
    "    labels = data_df[filter_col].values\n",
    "    \n",
    "    # compute silhouette value overall and for each corpus\n",
    "    data_df[\"silhouette\"] = metrics.silhouette_samples(X, labels, metric=metric, n_jobs=-1)\n",
    "        \n",
    "    overall_silhouette = np.mean(data_df[\"silhouette\"])\n",
    "    silhouette_bycorpus = data_df.groupby(filter_col).agg({\"silhouette\": \"mean\"}).reset_index()\n",
    "    \n",
    "    # compute intra-cluster measures\n",
    "    intra_cluster_measures = pd.DataFrame(intra_cluster_analysis(X, labels, metric)).T.reset_index()\n",
    "    \n",
    "    # combine data into one table\n",
    "    output = silhouette_bycorpus.merge(intra_cluster_measures, left_on=filter_col, right_on=\"index\",\n",
    "                                       how=\"inner\")\n",
    "    del output[\"index\"]\n",
    "    \n",
    "    # add overall silhouette\n",
    "    output = pd.concat([output, pd.DataFrame({\n",
    "        filter_col: [\"OVERALL\"],\n",
    "        \"silhouette\": [overall_silhouette],\n",
    "        \"avg\": [0],\n",
    "        \"max\": [0]\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    # cleanup\n",
    "    output.rename(columns={\"avg\": \"average_diameter_dist\", \"max\": \"complete_diameter_dist\"}, inplace=True)\n",
    "    \n",
    "    # save out file and return\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)        \n",
    "    output.to_csv(os.path.join(output_dir, output_filename) + \".csv\", index=False)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def fetch_analysis(input_dir, input_filename):\n",
    "    \"\"\"\n",
    "    Fetches a csv file with analysis data in it.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (string): directory where file is located. \n",
    "        input_filename (string): name of file to pull.\n",
    "        \n",
    "    Returns:\n",
    "        (Pandas DataFrame): dataframe of parsed csv file specified. \n",
    "        \n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(input_dir, input_filename) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Corpus-Level Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e51bc39a84409bad3ca28108dcb24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "facet = \"corpus\"\n",
    "for model_name, model_df in tqdm(model_df_dict.items()):\n",
    "    compute_cluster_similarity(data_df, model_df, facet, \"manhattan\", ANALYSIS_DIR,\n",
    "                               \"{0}_{1}-cluster-similarity\".format(model_name, facet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>average_diameter_dist</th>\n",
       "      <th>complete_diameter_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANC</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>662.529536</td>\n",
       "      <td>836.551697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KBEval</td>\n",
       "      <td>-0.004573</td>\n",
       "      <td>660.564252</td>\n",
       "      <td>826.244263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LUCorpus-v0.3</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>674.170072</td>\n",
       "      <td>844.236816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>-0.015831</td>\n",
       "      <td>665.564501</td>\n",
       "      <td>827.955383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NTI</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>637.321185</td>\n",
       "      <td>833.647095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PropBank</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>660.211510</td>\n",
       "      <td>828.987671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WikiTexts</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>656.214338</td>\n",
       "      <td>811.796570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OVERALL</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          corpus  silhouette  average_diameter_dist  complete_diameter_dist\n",
       "0            ANC    0.008203             662.529536              836.551697\n",
       "1         KBEval   -0.004573             660.564252              826.244263\n",
       "2  LUCorpus-v0.3   -0.011673             674.170072              844.236816\n",
       "3  Miscellaneous   -0.015831             665.564501              827.955383\n",
       "4            NTI    0.028401             637.321185              833.647095\n",
       "5       PropBank    0.007900             660.211510              828.987671\n",
       "6      WikiTexts    0.016205             656.214338              811.796570\n",
       "7        OVERALL    0.006814               0.000000                0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from biggest model\n",
    "filename = \"bert-large-nli-stsb-mean-tokens_corpus-cluster-similarity\"\n",
    "corpus_cluster_measures_df = fetch_analysis(ANALYSIS_DIR, filename)\n",
    "corpus_cluster_measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Document-Level Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaefe3d31064ed59588b9b9a4dbe7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "facet = \"document\"\n",
    "for model_name, model_df in tqdm(model_df_dict.items()):\n",
    "    compute_cluster_similarity(data_df, model_df, facet, \"manhattan\", ANALYSIS_DIR,\n",
    "                               \"{0}_{1}-cluster-similarity\".format(model_name, facet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>average_diameter_dist</th>\n",
       "      <th>complete_diameter_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110CYL067</td>\n",
       "      <td>-0.052127</td>\n",
       "      <td>639.936235</td>\n",
       "      <td>805.591003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110CYL068</td>\n",
       "      <td>-0.012957</td>\n",
       "      <td>617.426221</td>\n",
       "      <td>764.979309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110CYL069</td>\n",
       "      <td>-0.033120</td>\n",
       "      <td>626.213563</td>\n",
       "      <td>766.615051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110CYL070</td>\n",
       "      <td>-0.016470</td>\n",
       "      <td>616.122651</td>\n",
       "      <td>754.894592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110CYL072</td>\n",
       "      <td>-0.053550</td>\n",
       "      <td>645.051373</td>\n",
       "      <td>762.660522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>utd-icsi</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>607.506115</td>\n",
       "      <td>807.416504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>workAdvances</td>\n",
       "      <td>0.024419</td>\n",
       "      <td>577.842899</td>\n",
       "      <td>714.478333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>wsj_1640.mrg-NEW</td>\n",
       "      <td>0.024678</td>\n",
       "      <td>589.971900</td>\n",
       "      <td>721.458984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>wsj_2465</td>\n",
       "      <td>-0.025633</td>\n",
       "      <td>614.856052</td>\n",
       "      <td>776.315796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>OVERALL</td>\n",
       "      <td>-0.016818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             document  silhouette  average_diameter_dist  \\\n",
       "0           110CYL067   -0.052127             639.936235   \n",
       "1           110CYL068   -0.012957             617.426221   \n",
       "2           110CYL069   -0.033120             626.213563   \n",
       "3           110CYL070   -0.016470             616.122651   \n",
       "4           110CYL072   -0.053550             645.051373   \n",
       "..                ...         ...                    ...   \n",
       "98           utd-icsi   -0.010833             607.506115   \n",
       "99       workAdvances    0.024419             577.842899   \n",
       "100  wsj_1640.mrg-NEW    0.024678             589.971900   \n",
       "101          wsj_2465   -0.025633             614.856052   \n",
       "102           OVERALL   -0.016818               0.000000   \n",
       "\n",
       "     complete_diameter_dist  \n",
       "0                805.591003  \n",
       "1                764.979309  \n",
       "2                766.615051  \n",
       "3                754.894592  \n",
       "4                762.660522  \n",
       "..                      ...  \n",
       "98               807.416504  \n",
       "99               714.478333  \n",
       "100              721.458984  \n",
       "101              776.315796  \n",
       "102                0.000000  \n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from biggest model\n",
    "filename = \"bert-large-nli-stsb-mean-tokens_document-cluster-similarity\"\n",
    "corpus_cluster_measures_df = fetch_analysis(ANALYSIS_DIR, filename)\n",
    "corpus_cluster_measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Frame-Level Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccf2c05f99e4b76be7ebbd07e1834d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "facet = \"semantic_frame\"\n",
    "for model_name, model_df in tqdm(model_df_dict.items()):\n",
    "    compute_cluster_similarity(data_df, model_df, facet, \"manhattan\", ANALYSIS_DIR,\n",
    "                               \"{0}_{1}-cluster-similarity\".format(model_name, facet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic_frame</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>average_diameter_dist</th>\n",
       "      <th>complete_diameter_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abandonment</td>\n",
       "      <td>-0.249115</td>\n",
       "      <td>610.718471</td>\n",
       "      <td>676.590942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abounding_with</td>\n",
       "      <td>-0.196890</td>\n",
       "      <td>652.660740</td>\n",
       "      <td>774.395813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abundance</td>\n",
       "      <td>-0.255572</td>\n",
       "      <td>653.900269</td>\n",
       "      <td>653.900269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abusing</td>\n",
       "      <td>0.061572</td>\n",
       "      <td>466.111694</td>\n",
       "      <td>466.111694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accompaniment</td>\n",
       "      <td>-0.242196</td>\n",
       "      <td>671.516797</td>\n",
       "      <td>764.844360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Win_prize</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Withdraw_from_participation</td>\n",
       "      <td>-0.060547</td>\n",
       "      <td>515.155699</td>\n",
       "      <td>646.293091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Within_distance</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Work</td>\n",
       "      <td>-0.186387</td>\n",
       "      <td>636.969460</td>\n",
       "      <td>765.061157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>OVERALL</td>\n",
       "      <td>-0.204405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  semantic_frame  silhouette  average_diameter_dist  \\\n",
       "0                    Abandonment   -0.249115             610.718471   \n",
       "1                 Abounding_with   -0.196890             652.660740   \n",
       "2                      Abundance   -0.255572             653.900269   \n",
       "3                        Abusing    0.061572             466.111694   \n",
       "4                  Accompaniment   -0.242196             671.516797   \n",
       "..                           ...         ...                    ...   \n",
       "793                    Win_prize    0.000000                    NaN   \n",
       "794  Withdraw_from_participation   -0.060547             515.155699   \n",
       "795              Within_distance    0.000000                    NaN   \n",
       "796                         Work   -0.186387             636.969460   \n",
       "797                      OVERALL   -0.204405               0.000000   \n",
       "\n",
       "     complete_diameter_dist  \n",
       "0                676.590942  \n",
       "1                774.395813  \n",
       "2                653.900269  \n",
       "3                466.111694  \n",
       "4                764.844360  \n",
       "..                      ...  \n",
       "793                0.000000  \n",
       "794              646.293091  \n",
       "795                0.000000  \n",
       "796              765.061157  \n",
       "797                0.000000  \n",
       "\n",
       "[798 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from biggest model\n",
    "filename = \"bert-large-nli-stsb-mean-tokens_semantic_frame-cluster-similarity\"\n",
    "corpus_cluster_measures_df = fetch_analysis(ANALYSIS_DIR, filename)\n",
    "corpus_cluster_measures_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
